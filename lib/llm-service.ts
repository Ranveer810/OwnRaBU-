import { GoogleGenAI, HarmCategory, HarmBlockThreshold, Tool, Type } from "@google/genai";
import { Message, Sender, ToolInvocation, LLMSettings, LLMModel } from "../types";

// --- System Instruction ---
const SYSTEM_INSTRUCTION = `
You are Zenith, an expert Frontend AI Coding Agent.
Your goal is to help users build beautiful, functional, and modern websites using HTML, CSS, and JavaScript.

CAPABILITIES:
1. **generate_new_project**: Create a full new project from scratch (index.html, styles.css, script.js).
2. **read_files**: Read the current content of the files. Use this whenever you need to understand the current code state.
3. **update_file**: COMPLETELY replace the content of a single file (html, css, or javascript).
4. **patch_file**: INTELLIGENTLY replace a specific part of a file using search and replace strings. Use this for small edits.
5. **screenshot_website**: Capture a visual screenshot of the current website.
6. **validate_functionality**: Run an automated test script on the current website.
7. **read_console_logs**: Read the browser console logs generated by the website. Use this to debug JavaScript errors.

RULES:
- Always strive for modern, responsive designs using Tailwind CSS.
- When the user asks to change something small, PREFER using \`patch_file\`.
- When using \`patch_file\`, ensure the \`search_string\` matches EXACTLY what is in the code.
- 'index.html' must be a complete HTML5 structure.
- If the user mentions an error or that something isn't working, check the console logs first using \`read_console_logs\`.
`;

// --- Tool Definitions (Unified) ---

const GOOGLE_TOOLS: Tool[] = [{
  functionDeclarations: [
    {
      name: "read_files",
      description: "Read the full content of the current project files.",
    },
    {
      name: "update_file",
      description: "Completely replace the content of a single file",
      parameters: {
        type: Type.OBJECT,
        properties: {
          target: { type: Type.STRING, enum: ["html", "css", "javascript"], description: "The file to update" },
          content: { type: Type.STRING, description: "The full new content of the file" }
        },
        required: ["target", "content"]
      }
    },
    {
      name: "patch_file",
      description: "Replace a specific segment of code within a file",
      parameters: {
        type: Type.OBJECT,
        properties: {
          target: { type: Type.STRING, enum: ["html", "css", "javascript"], description: "The file to patch" },
          search_string: { type: Type.STRING, description: "The exact code segment to find and replace" },
          replacement_string: { type: Type.STRING, description: "The new code to insert" }
        },
        required: ["target", "search_string", "replacement_string"]
      }
    },
    {
      name: "screenshot_website",
      description: "Take a visual screenshot of the current rendered website.",
      parameters: { type: Type.OBJECT, properties: {}, required: [] }
    },
    {
      name: "validate_functionality",
      description: "Execute a JavaScript test script against the current website.",
      parameters: {
        type: Type.OBJECT,
        properties: {
          test_script: { type: Type.STRING, description: "JavaScript code that asserts conditions." }
        },
        required: ["test_script"]
      }
    },
    {
      name: "read_console_logs",
      description: "Read the browser console logs (errors, warnings, logs) from the preview window.",
      parameters: { type: Type.OBJECT, properties: {}, required: [] }
    }
  ]
}];

// Helper to convert Google GenAI Schema (Uppercase types) to OpenAI JSON Schema (Lowercase types)
function convertSchemaToOpenAI(schema: any): any {
  if (!schema) return undefined;
  
  const newSchema = { ...schema };
  
  // Convert Type Enum to lowercase string (e.g., "STRING" -> "string")
  if (newSchema.type) {
      newSchema.type = newSchema.type.toLowerCase();
  }
  
  // Recursively convert properties
  if (newSchema.properties) {
      const newProps: any = {};
      for (const key in newSchema.properties) {
          newProps[key] = convertSchemaToOpenAI(newSchema.properties[key]);
      }
      newSchema.properties = newProps;
  }
  
  // Recursively convert items (for arrays)
  if (newSchema.items) {
      newSchema.items = convertSchemaToOpenAI(newSchema.items);
  }
  
  return newSchema;
}

// OpenAI / Groq Compatible Tools Format
const OPENAI_TOOLS = GOOGLE_TOOLS[0].functionDeclarations?.map(tool => {
  const parameters = (tool.parameters as any);
  
  return {
    type: "function",
    function: {
      name: tool.name,
      description: tool.description,
      parameters: parameters ? convertSchemaToOpenAI(parameters) : { type: "object", properties: {} }
    }
  };
});

// --- Types ---

type StreamChunk = {
  text?: string;
  toolCall?: ToolInvocation;
  toolResult?: { toolCallId: string; result: any };
};

// --- API Helpers ---

export async function getAvailableModels(settings: LLMSettings): Promise<LLMModel[]> {
  const provider = settings.selectedProvider;
  const config = settings[provider];

  if (!config.apiKey) return [];

  try {
    if (provider === 'google') {
      // Return curated list for Google
      return [
        { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash' },
        { id: 'gemini-2.0-flash-lite-preview-02-05', name: 'Gemini 2.0 Flash Lite' },
        { id: 'gemini-1.5-pro', name: 'Gemini 1.5 Pro' },
        { id: 'gemini-1.5-flash', name: 'Gemini 1.5 Flash' },
      ];
    } else if (provider === 'groq') {
      const res = await fetch('https://api.groq.com/openai/v1/models', {
        headers: { 'Authorization': `Bearer ${config.apiKey}` }
      });
      const data = await res.json();
      return data.data.map((m: any) => ({ id: m.id, name: m.id }));
    } else if (provider === 'openai') {
      const baseUrl = config.baseUrl || 'https://api.openai.com/v1';
      const res = await fetch(`${baseUrl}/models`, {
        headers: { 'Authorization': `Bearer ${config.apiKey}` }
      });
      const data = await res.json();
      return data.data.map((m: any) => ({ id: m.id, name: m.id }));
    }
  } catch (e) {
    console.error("Failed to fetch models", e);
    return [];
  }
  return [];
}

// --- Main Chat Function ---

export async function* streamChat(
  history: Message[],
  newMessage: string,
  settings: LLMSettings,
  toolExecutor: (name: string, args: any) => Promise<any>,
  signal?: AbortSignal
): AsyncGenerator<StreamChunk, void, unknown> {

  if (settings.selectedProvider === 'google') {
    yield* streamGoogleChat(history, newMessage, settings, toolExecutor, signal);
  } else {
    yield* streamOpenAICompatibleChat(history, newMessage, settings, toolExecutor, signal);
  }
}

// --- Google Implementation ---

async function* streamGoogleChat(
  history: Message[],
  newMessage: string,
  settings: LLMSettings,
  toolExecutor: (name: string, args: any) => Promise<any>,
  signal?: AbortSignal
): AsyncGenerator<StreamChunk, void, unknown> {
  
  const config = settings.google;
  const ai = new GoogleGenAI({ apiKey: config.apiKey });
  
  const chatHistory = history
    .filter((msg) => msg.role !== Sender.SYSTEM)
    .map((msg) => ({
      role: msg.role === Sender.USER ? "user" : "model",
      parts: [{ text: msg.content }],
    }));

  const chat = ai.chats.create({
    model: config.model || 'gemini-2.0-flash',
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      tools: GOOGLE_TOOLS,
      temperature: 0.7,
    },
    history: chatHistory,
  });

  let currentMessage = newMessage;

  while (true) {
    if (signal?.aborted) break;

    // @ts-ignore - Dynamic message type handling
    const result = await chat.sendMessageStream({ message: currentMessage });
    
    let toolCallAccumulator: any[] = [];

    try {
      for await (const chunk of result) {
        if (signal?.aborted) break;

        const text = chunk.text;
        if (text) yield { text };
        
        const calls = chunk.functionCalls;
        if (calls && calls.length > 0) {
          toolCallAccumulator.push(...calls);
        }
      }
    } catch (e) {
      if (signal?.aborted) break;
      throw e;
    }

    if (toolCallAccumulator.length === 0 || signal?.aborted) break;

    const toolResponses = [];
    for (const call of toolCallAccumulator) {
      if (signal?.aborted) break;

      const stableCallId = call.id || Math.random().toString(36).substring(2, 10);
      
      yield { 
        toolCall: {
          toolCallId: stableCallId,
          toolName: call.name,
          args: call.args
        } 
      };
      
      try {
        const functionResponse = await toolExecutor(call.name, call.args);
        
        yield {
          toolResult: {
            toolCallId: stableCallId,
            result: functionResponse
          }
        };

        toolResponses.push({
          id: call.id || stableCallId,
          name: call.name,
          response: { result: functionResponse }
        });
      } catch (err: any) {
        toolResponses.push({
          id: call.id || stableCallId,
          name: call.name,
          response: { error: err.message }
        });
      }
    }

    if (signal?.aborted) break;

    const responseParts = toolResponses.map(tr => ({
        functionResponse: {
            name: tr.name,
            response: tr.response,
            id: tr.id
        }
    }));
    
    currentMessage = responseParts as any; 
  }
}

// --- OpenAI / Groq Implementation ---

async function* streamOpenAICompatibleChat(
  history: Message[],
  newMessage: string,
  settings: LLMSettings,
  toolExecutor: (name: string, args: any) => Promise<any>,
  signal?: AbortSignal
): AsyncGenerator<StreamChunk, void, unknown> {
  
  const provider = settings.selectedProvider;
  const config = settings[provider]; // Safely access nested config

  const baseUrl = provider === 'groq' 
    ? 'https://api.groq.com/openai/v1' 
    : (config.baseUrl || 'https://api.openai.com/v1');

  // Convert History to OpenAI format
  const messages = [
    { role: "system", content: SYSTEM_INSTRUCTION },
    ...history.map(msg => ({
        role: msg.role === Sender.USER ? "user" : "assistant",
        content: msg.content
    })),
    { role: "user", content: newMessage }
  ];

  let keepGoing = true;

  while (keepGoing) {
    if (signal?.aborted) break;
    keepGoing = false;

    try {
      const response = await fetch(`${baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${config.apiKey}`
        },
        body: JSON.stringify({
          model: config.model,
          messages: messages,
          stream: true,
          tools: OPENAI_TOOLS,
          tool_choice: "auto"
        }),
        signal: signal
      });

      if (!response.ok) {
          const err = await response.text();
          yield { text: `Error: ${response.status} - ${err}` };
          return;
      }

      if (!response.body) return;

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';
      let toolCallsMap: Record<number, any> = {};
      let finishReason = null;

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.trim() === '' || line.trim() === 'data: [DONE]') continue;
          if (!line.startsWith('data: ')) continue;

          try {
            const json = JSON.parse(line.substring(6));
            const delta = json.choices[0].delta;
            finishReason = json.choices[0].finish_reason;

            if (delta.content) {
              yield { text: delta.content };
            }

            if (delta.tool_calls) {
              for (const tc of delta.tool_calls) {
                const index = tc.index;
                if (!toolCallsMap[index]) toolCallsMap[index] = { id: tc.id, name: '', args: '' };
                if (tc.id) toolCallsMap[index].id = tc.id;
                if (tc.function?.name) toolCallsMap[index].name += tc.function.name;
                if (tc.function?.arguments) toolCallsMap[index].args += tc.function.arguments;
              }
            }
          } catch (e) {
            console.error("Error parsing stream", e);
          }
        }
      }

      if (signal?.aborted) break;

      // Process Tool Calls if any
      const toolCalls = Object.values(toolCallsMap);
      
      if (toolCalls.length > 0) {
        // Append assistant's tool call message to history
        messages.push({
          role: "assistant",
          content: null,
          tool_calls: toolCalls.map(tc => ({
            id: tc.id,
            type: 'function',
            function: { name: tc.name, arguments: tc.args }
          }))
        } as any);

        for (const tc of toolCalls) {
           if (signal?.aborted) break;

           const args = JSON.parse(tc.args);
           yield {
              toolCall: {
                  toolCallId: tc.id,
                  toolName: tc.name,
                  args: args
              }
           };

           try {
               const result = await toolExecutor(tc.name, args);
               yield {
                   toolResult: {
                       toolCallId: tc.id,
                       result: result
                   }
               };
               
               // Append tool result to history
               messages.push({
                   role: "tool",
                   tool_call_id: tc.id,
                   content: JSON.stringify(result)
               } as any);

           } catch (e: any) {
               messages.push({
                   role: "tool",
                   tool_call_id: tc.id,
                   content: JSON.stringify({ error: e.message })
               } as any);
           }
        }
        // Loop back to send results to model
        keepGoing = true;
      }
    } catch (e: any) {
       if (e.name === 'AbortError') {
         yield { text: '\n\n[Stopped by user]' };
         return;
       }
       yield { text: `\n\nError: ${e.message}` };
       return;
    }
  }
}